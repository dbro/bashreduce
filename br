#!/bin/bash
# bashreduce: mapreduce in bash
# erik@fawx.com

# DB notes:
#
# canonical mapreduce process flow, according to wikipedia
# 1. input reader: [(k1,v1)]
# 2. map: [(k1,v1)] -> [(k2,v2)]
# 3. shuffle (partition + compare)
# 4. reduce: (k2, [v2]) -> (k2, v3), where v3 might be another list
# 5. output writer

# 1a. input reader. master creates (k,v) pairs for distribution to mappers
# 1b. data received by all mappers
# 2. each mapper performs map for each (k,v) -> (k',v')
# 3. shuffle to determine next destination for the data
# 3a. partition (k,total#) -> #index of reducer
# 3b. compare (k1,k2) -> k1>k2 ? used to sort before reduce begins
# 4a. each mapper sends partitioned, sorted (k',v') data to next destination
# 4b. next destination merges the data for each k' from all mappers
# 5. next destination gets instructions on what to do with the data
#    (map, partition, compare)
# 6. output writer. master stores (k', v") permanently
#
# in practice, this would be set up as an order list of tuples:
# [(m,p,c)] where each tuple represents a stage of processing
# and each node is able to send data to all the others
# there doesn't need to be a master node if all of the steps
# are known at all times by all nodes, and all nodes are ready
# to give a "no data from me" message to the nodes that depend
# on them.
# what about failure/errors? those need to be coordinated by a master
#
# for example, this is a processing list for visitor trial/success counting:
# 0m: key1=none, val1=pathspec, map=find {input_folder} -name {filespec} -> [(k2,v2)]=[({input_file},none)]
# 0p: hash(key2) % {number_of_workers}
# 0c: no sort needed for each key2 set (because there is only 1 member)
# 1m: key2 = visitor_id+timestamp, val2 = $0
# 1p: hash({visitor_id_part_of_key}) % {number_of_workers}
# 1c: for each partition, sort by key2
# 2m: session_analysis(val) -> k2=testcase v2=visitor_id,success_state [this is a reduce step, which assumes the input is sorted appropriately. it accumulates state as it reads sequentially]
# 2p: =1 (send all to node1 because we expect it to be a small set)
# 2c: sort by key2, alpha
# 3m: summary_stats(val) -> k2=k1, v2=total counts, statistical significance test info [this is another reduce step that assumes the input is sorted appropriately. it accumulates state as it reads sequentially]
# 3p: =1
# 3c: =1
# 4: output writer. should cache results based on input parameters and source file state to speed up subsequent requests for the same information.

#each step specifies:
# ?-i input files
# -m map process. it is expected to know how to parse the records from the input data
#    and the k1 from each record
# -k key2 extraction function. default 'cut -f1'
# -p partition function. input is key2 and list of outputs
# -c compare function. for sorting each output from partition by key2
#each step produces a set of files, one for each partition. it tells the master when it's done
#and where the files it produced can be found. and includes log information about who it is, what it was asked to do, the input data, start/stop time and any errors or debug info

# br questions/limits
# line length limit of 8kb (buffer)
# all white space treated as delimiter
# FNV hash uses v1 instead of v1a
# possible ssh overhead when using localhost
# sort order appears to be inconsistent so merge function may not work properly
# the "reduce" is really just part 2 of the map;
# shuffle comes after "reduce" and before "merge" programs, so it's not really
# a distributed reduce step. all the reduction happens in the local/master node

# a proper mapreduce might work like this:
# specify the location of the input data (default stdin, but can be files or directory)
# specify hosts that will do the work
# specify a map stage field delimiter (default tab)
# specify a map stage key index (default 1)
# specify the map function
# the master partitions the data to give all data for each key to a single node for mapping
# use the entire row as the value in the (k,v) tuple when mapping
# specify a shuffle stage field delimiter (default tab)
# specify a shuffle stage key index (default 1)
# each mapper node partitions its mapped data to group all data for each k' together for reducing
# specify a shuffle stage comparison function to sort [v'] for each k'
# each mapper node sorts its mapped data
# each mapper node transmits its mapped data to the appropriate reducer
# specify the reduce function
# each reducer receives the mapped data and merges the sorted data for each k' together
# use the entire row as the value in the (k,v) tuple when reducing
# specify the location of the output data (default stdout)

# can the map and reduce phases be handled similarly? the main difference appears to be the way
# the input arrives (from one source or from multiple sources). probably makes sense to bring
# all data back from the mappers to the master before starting any reduce jobs. In that case,
# it could be a simple concatenation of 2 mappings, where the second "map" is actually a reduce?

# should each mapper be able to send data directly to the reducers?
# probably not, because the reducer won't know when it's got a complete set of data

# partition: (k, #total) -> #index
# compare: ((k1,v1),(k2,v2)) -> -1/0/1
# map: (k,v) -> [(k',v')]
# reduce: (k, [v]) -> (k, v)  -- this is sometimes imposed as a constraint to allow cascading reduce functions
# reduce: (k, [v]) -> (k, v') -- and this is also ok. see http://en.wikipedia.org/wiki/Fold_(higher-order_function) which mentions asymmetry of types
# in other words, in the reduce phase each "value" is a sorted list of items

# so in the visitor trial/success counting, the stages might be:
# 1. collect all requests for each visitor id, and sort by timestamp (re-sort)
# 2. determine the test+cases and success stats for each visitor id (map, assuming sorted input)
# 3. summarize the stats for each test+case (map)



usage() {
	echo "Usage: $1 [-h '<host>[ <host>][...]'] [-f]" >&2
	echo "       [-m <map>] [-r <reduce>] [-M <merge>]" >&2
	echo "       [-i <input>] [-o <output>] [-c <column>]" \
		"[-S <sort-mem-MB>]" >&2
	echo "       [-t <tmp>] [-?]" >&2
	if [ -n "$2" ] ; then
		echo "  -h hosts to use; repeat hosts for multiple cores" >&2
		echo "     (defaults to contents of /etc/br.hosts)" >&2
		echo "  -f send filenames, not data, over the network," >&2
		echo "     implies each host has a mirror of the dataset" >&2
		echo "  -m map program (effectively defaults to cat)" >&2
		echo "  -r reduce program. receives output from map, still on the same host" >&2
        # DB: why is the reduce program broken out here? it could be included with the map command
        # and the suffle is always a sort on the column specified by -c ?
		echo "  -M merge program. receives sorted output from all hosts' map-reduce" >&2
		echo "  -i input file or directory (defaults to stdin)" >&2
		echo "  -o output file (defaults to stdout)" >&2
		echo "  -c column used by sort (defaults to 1)" >&2
		echo "  -S memory to use for sort (defaults to 256M)" >&2
		echo "  -t tmp directory (defaults to /tmp)" >&2
		echo "  -? this help message" >&2
	fi
	exit 2
}

# Defaults
hosts=
filenames=false
map=
reduce=
merge=
input=
output=
column=1
sort_mem=256M
tmp=/tmp

# Dependencies
PWD=`pwd`
# netcat-traditional (because netcat-openbsd does not support "-l" parameter)
if which nc.traditional >>$tmp/br_stderr; then
	NC=$(which nc.traditional)
elif [[ -f brutils/nc.traditional ]]; then
	NC=$PWD/brutils/nc.traditional
else
    echo "warning. can't find required version of netcat. attempting to use default nc"
    NC=$(which nc)
fi
LC_order='C' # use strict C-style character sort order

program=$(basename $0)
while getopts "h:fm:r:M:i:o:c:S:t:?" name; do
	case "$name" in
		h) hosts=$OPTARG;;
		f) filenames=true;;
		m) map=$OPTARG;;
		r) reduce=$OPTARG;;
		M) merge=$OPTARG;;
		i) input=$OPTARG;;
		o) output=$OPTARG;;
		c) column=$OPTARG;;
		S) sort_mem=$OPTARG;;
		t) tmp=$OPTARG;;
		?) usage $program MOAR;;
		*) usage $program;;
	esac
done 

# If -h wasn't given, try /etc/br.hosts
if [[ -z "$hosts" ]]; then
	if [[ -e /etc/br.hosts ]]; then
		hosts=$(cat /etc/br.hosts)
	else
		echo "$program: must specify -h or provide /etc/br.hosts"
		usage $program
	fi
fi

# Start br_stderr from a clean slate
cp /dev/null $tmp/br_stderr

# Setup map and reduce as parts of a pipeline
[[ -n "$map" ]] && map="| $map 2>>$tmp/br_stderr"
[[ $filenames == true ]] && map="| xargs -0 -n1 \
	sh -c 'zcat \$0 2>>$tmp/br_stderr || cat \$0 2>>$tmp/br_stderr' $map"
[[ -n "$reduce" ]] && reduce="| $reduce 2>>$tmp/br_stderr"

jobid="$(uuidgen)"
jobpath="$tmp/br_job_$jobid"
nodepath="$tmp/br_node_$jobid"
mkdir -p $jobpath/{in,out}

port_in=8192
port_out=$(($port_in + 1))
host_idx=0
out_files=
for host in $hosts; do
	mkfifo $jobpath/{in,out}/$host_idx

	# Listen for work (remote)
	ssh -n $host "mkdir -p $nodepath/"
	pid=$(ssh -n $host "$NC -l -p $port_out >$nodepath/$host_idx \
		2>>$tmp/br_stderr </dev/null & jobs -l" \
		| awk {'print $2'})

	# Do work (remote)
    # DB: changed LC_ALL to LC_C here to match sort order in brm
	ssh -n $host "tail -s0.1 -f --pid=$pid $nodepath/$host_idx \
		2>>$tmp/br_stderr </dev/null \
		| env LC_ALL='$LC_order' sort -S$sort_mem -T$tmp -k$column,$column \
		2>>$tmp/br_stderr \
		$map $reduce \
		| $NC -q0 -l -p $port_in >>$tmp/br_stderr &"

	# Send work (local)
	$NC $host $port_in >$jobpath/in/$host_idx &

	# Receive results (local)
	$NC -q0 $host $port_out <$jobpath/out/$host_idx &
	out_files="$out_files $jobpath/out/$host_idx"

	# ++i
	port_in=$(($port_in + 2))
	port_out=$(($port_in + 1))
	host_idx=$(($host_idx + 1))

done

# Create the command to produce input
if [[ -d "$input" ]]; then
	input="find $input -type f -print0 |"
	[[ $filenames == false ]] && input="$input xargs -0 -n1 \
		sh -c 'zcat \$0 2>>$tmp/br_stderr || cat \$0 2>>$tmp/br_stderr' |"
elif [[ -f "$input" ]]; then
	input="sh -c 'zcat $input 2>>$tmp/br_stderr \
		|| cat $input 2>>$tmp/br_stderr' |"
else
	input=
fi

# Partition local input to the remote workers
if which brp >>$tmp/br_stderr; then
	BRP=brp
elif [[ -f brutils/brp ]]; then
	BRP=brutils/brp
fi
if [[ -n "$BRP" ]]; then
	eval "$input $BRP - $(($column - 1)) $out_files"
else
	# use awk if we don't have brp
	# we're taking advantage of a special property that awk leaves its file handles open until its done
	# i think this is universal
	# we're also sending a zero length string to all the handles at the end, in case some pipe got no love
	eval "$input awk '{
			srand(\$$column);
			print \$0 >>\"$jobpath/out/\"int(rand() * $host_idx);
		}
		END {
			for (i = 0; i != $host_idx; ++i)
				printf \"\" >>\"$jobpath/out/\"i;
		}'"
fi

# Merge output from hosts into one
#   Maybe use the -M program, if not just sort (preferring brm)
if which brm >>$tmp/br_stderr; then
	BRM=brm
elif [[ -f brutils/brm ]]; then
	BRM=brutils/brm
fi
if [[ -n "$merge" ]]; then
    # DB: by using cat here, that breaks the requirement that the input
    # is sorted -- because it's all concatenated into one long stream
    echo "using custom merge command" >&2
	eval "find $jobpath/in -type p -print0 | xargs -0 cat \
		| $merge 2>>$tmp/br_stderr ${output:+| pv >$output}"
else
	if [[ -n "$BRM" ]]; then
        echo "using brm to merge" >&2
		eval "$BRM - $(($column - 1)) $(find $jobpath/in/ -type p -print0 | xargs -0) \
			${output:+| pv >$output}"
	else
        echo "using sort -m to merge" >&2
		# use sort -m if we don't have brm
		# sort -m creates tmp files if too many input files are specified
		# brm doesn't do this
		eval "env LC_ALL='$LC_order' sort -k$column,$column -S$sort_mem -m $jobpath/in/* \
			${output:+| pv >$output}"
	fi
fi

# Cleanup
rm -rf $jobpath
for host in $hosts; do
	ssh $host "rm -rf $nodepath"
done

# TODO: is there a safe way to kill subprocesses upon fail?
# this seems to work: /bin/kill -- -$$
